{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mkul3JPk4v2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "\n",
        "  - Simple Linear Regression is a basic statistical method used\n",
        "    to model the relationship between two variables:\n",
        "    One independent variable (predictor or input) — often denoted as X\n",
        "    One dependent variable (response or output) — often denoted as Y.\n",
        "\n",
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "\n",
        "  - The key assumptions of Simple Linear Regression are super\n",
        "    important because if they’re violated, the model's predictions and inferences (like hypothesis tests and confidence intervals) may not be valid.\n",
        "\n",
        "3. What does the coefficient m represent in the equation Y=mX+c?\n",
        "\n",
        "  - The coefficient m represents the slope of the line.\n",
        "    It tells us how much Y changes for a one-unit change in X.\n",
        "\n",
        "4. What does the intercept c represent in the equation Y=mX+c?\n",
        "\n",
        "  - It represents the starting point or baseline value of the\n",
        "    dependent variable before any effect of the independent variable.\n",
        "5.  How do we calculate the slope m in Simple Linear Regression?\n",
        "\n",
        "  - Calculating the slope 𝑚\n",
        "    m in Simple Linear Regression is all about finding the best-fitting straight line through the data points.\n",
        "     In Simple Linear Regression, the slope\n",
        "𝑚\n",
        "m (also called\n",
        "𝛽\n",
        "1\n",
        "β\n",
        "1\n",
        "​\n",
        " ) represents the change in the dependent variable\n",
        "𝑦\n",
        "y for a one-unit change in the independent variable\n",
        "𝑥\n",
        "x.\n",
        "\n",
        "6. What is the purpose of the least squares method in Simple\n",
        "   Linear Regression?\n",
        "\n",
        " - The least squares method in Simple Linear Regression is used\n",
        "   to find the best-fitting line through a set of data points by minimizing the total error between the predicted values and the actual values.\n",
        "\n",
        "7.  How is the coefficient of determination (R²) interpreted in\n",
        "    Simple Linear Regression?\n",
        "\n",
        "  - Great question! The coefficient of determination, written as\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  (R-squared), is a key metric in Simple Linear Regression that tells you how well your regression line explains the variation in the dependent variable\n",
        "𝑦\n",
        "y.\n",
        "\n",
        "8. What is Multiple Linear Regression?\n",
        "\n",
        "  - Multiple Linear Regression (MLR) is an extension of simple linear regression that models the relationship between a dependent variable\n",
        "𝑦\n",
        "y and two or more independent variables (predictors)\n",
        "𝑥\n",
        "1\n",
        ",\n",
        "𝑥\n",
        "2\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑥\n",
        "𝑛\n",
        "x\n",
        "1\n",
        "​\n",
        " ,x\n",
        "2\n",
        "​\n",
        " ,…,x\n",
        "n\n",
        "​\n",
        " .\n",
        "\n",
        "9.  What is the main difference between Simple and Multiple\n",
        "    Linear Regression?\n",
        "\n",
        "   - Simple Linear Regression:\n",
        "\n",
        "    One independent variable (predictor).\n",
        "\n",
        "    The relationship between\n",
        "𝑦\n",
        "y (dependent variable) and\n",
        "𝑥\n",
        "x (independent variable) is modeled.\n",
        "\n",
        "   Equation:\n",
        "𝑦\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "+\n",
        "𝜖\n",
        "y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x+ϵ\n",
        "\n",
        "   Multiple Linear Regression:\n",
        "\n",
        "   Two or more independent variables (predictors).\n",
        "\n",
        "   The relationship between\n",
        "𝑦\n",
        "y and multiple\n",
        "𝑥\n",
        "x's is modeled.\n",
        "\n",
        "   Equation:\n",
        "𝑦\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "1\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "+\n",
        "𝜖\n",
        "y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x\n",
        "1\n",
        "​\n",
        " +β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        "​\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        "​\n",
        " +ϵ\n",
        "\n",
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "\n",
        "  - In Multiple Linear Regression (MLR), there are several key\n",
        "    assumptions that need to be met for the model to provide valid results. These assumptions ensure that the regression analysis produces reliable and meaningful predictions.\n",
        "\n",
        "11. What is heteroscedasticity, and how does it affect the\n",
        "    results of a Multiple Linear Regression model?\n",
        "\n",
        "  - Heteroscedasticity refers to a situation in a Multiple\n",
        "    Linear Regression (MLR) model where the variance of the errors (residuals) is not constant across all levels of the independent variables.\n",
        "    Effects of Heteroscedasticity on the Model:\n",
        "    i.Inefficient Estimators\n",
        "    ii.Invalid Statistical Tests\n",
        "    iii.Distorted Predictions\n",
        "    iv. Bias in Goodness-of-Fit Measures\n",
        "\n",
        "12.  How can you improve a Multiple Linear Regression model  \n",
        "     with high multicollinearity?\n",
        "\n",
        "  - High multicollinearity in a Multiple Linear Regression (MLR) model occurs when two or more independent variables are highly correlated with each other. This makes it difficult to determine the individual effect of each predictor on the dependent variable, and can lead to:\n",
        "\n",
        "  Inflated standard errors of the regression coefficients, making them statistically insignificant.\n",
        "\n",
        "  Unstable coefficient estimates, where small changes in the data can lead to large changes in the model.\n",
        "\n",
        "  Overfitting, where the model becomes too complex and captures noise rather than the true underlying pattern.\n",
        "\n",
        "13. What are some common techniques for transforming  \n",
        "    categorical variables for use in regression models?\n",
        "\n",
        "  - Transforming categorical variables for use in regression\n",
        "    models is essential because regression models typically require numerical input. Here are some common techniques.\n",
        "    i. One-Hot Encoding\n",
        "    ii. Label Encoding\n",
        "    iii. Ordinal Encoding\n",
        "    iv. Binary Encoding\n",
        "    v. Frequency or Count Encoding\n",
        "    vi. Target Encoding (Mean Encoding)\n",
        "\n",
        "14.  What is the role of interaction terms in Multiple Linear\n",
        "     Regression?\n",
        "    - Interaction terms in Multiple Linear Regression (MLR)  \n",
        "      play a crucial role in modeling the relationship between the independent variables and the dependent variable by accounting for the combined effect of two or more predictors.\n",
        "\n",
        "15. How can the interpretation of intercept differ between\n",
        "    Simple and Multiple Linear Regression?\n",
        "\n",
        "  - The intercept is essential for correctly positioning the regression plane/line, but its real-world interpretation depends on whether the zero values of the predictors are meaningful or not.\n",
        "  If zero is not meaningful, the intercept may still be statistically valid, but not interpretable in practical terms.\n",
        "\n",
        "16. What is the significance of the slope in regression\n",
        "   analysis, and how does it affect predictions?\n",
        "\n",
        "  - The slope in regression analysis is one of the most\n",
        "    important components of a regression model — it tells us how much the dependent variable (Y) is expected to change when an independent variable (X) changes by one unit.\n",
        "\n",
        "    How It Affects Predictions\n",
        "    The slope is directly used to make predictions. For example, if your model is:\n",
        "\n",
        "    House Price=50,000 + 100\n",
        "⋅\n",
        "    Square Footage\n",
        "    House Price=50,000+100⋅Square Footage\n",
        "    This means every additional square foot adds $100 to the predicted price.\n",
        "\n",
        "    If a house is 2,000 sq ft, the predicted price is:\n",
        "    50,000 + 100×2000 = 250,000\n",
        "    So, the slope drives the change in the predicted value of Y as X changes.\n",
        "\n",
        "17.  How does the intercept in a regression model provide\n",
        "     context for the relationship between variables?\n",
        "\n",
        "    - The intercept in a regression model gives us the starting\n",
        "      point or baseline value of the dependent variable (Y) when all independent variables (X) are zero. While it doesn’t always have a practical meaning, it helps anchor the regression line or plane and provides context for understanding the effect of the other variables.\n",
        "\n",
        "18. What are the limitations of using R² as a sole measure of\n",
        "    model performance?\n",
        "  - Limitations of Using R² Alone\n",
        "    i.  Doesn't Indicate Model Accuracy\n",
        "    ii. Doesn’t Tell You If the Model Is Appropriate\n",
        "    iii. Sensitive to Overfitting\n",
        "    iv.  Doesn’t Reflect Predictive Power on New Data\n",
        "    v. Can Be Misleading with Nonlinear Relationships\n",
        "    vi. No Standard for \"Good\" R²\n",
        "\n",
        "19. How would you interpret a large standard error for a\n",
        "    regression coefficient?\n",
        "\n",
        "  - A large standard error for a regression coefficient is a  \n",
        "    red flag that tells you something’s not quite stable or reliable about your estimate.\n",
        "\n",
        "20. How can heteroscedasticity be identified in residual plots,\n",
        "    and why is it important to address it?\n",
        "\n",
        "  - Pattern | What It Suggests\n",
        "   🔸 Funnel Shape (widening or narrowing) | Classic sign of heteroscedasticity\n",
        "   🔸 Curved or wave patterns | Indicates non-linearity or other issues\n",
        "   🔹 Random scatter with no clear shape | Good! Suggests homoscedasticity (ideal)\n",
        "\n",
        "   Test | Description\n",
        "   Breusch-Pagan test | Checks for linear relationship between residual variance and predictors\n",
        "   White test | More general — detects many types of heteroscedasticity\n",
        "   Goldfeld-Quandt test | Compares variances of residuals in different parts of the dataset\n",
        "\n",
        "   Biased Standard Errors\n",
        "  → This leads to incorrect p-values, which means you might:\n",
        "\n",
        "   Think a variable is significant when it isn’t (Type I error), or\n",
        "\n",
        "   Miss a real effect (Type II error).\n",
        "\n",
        "   Confidence Intervals & Hypothesis Tests Become Invalid\n",
        "   → Your conclusions might be misleading or wrong.\n",
        "\n",
        "   Reduced Efficiency\n",
        "   → The OLS estimator is no longer the Best Linear Unbiased Estimator (BLUE).\n",
        "\n",
        "21. What does it mean if a Multiple Linear Regression model   \n",
        "    has a high R² but low adjusted R²?\n",
        "\n",
        "  - A high R² but low adjusted R² in a Multiple Linear\n",
        "    Regression model is a classic sign that your model might be overfitting or bloated with irrelevant predictors.\n",
        "\n",
        "22.  Why is it important to scale variables in Multiple Linear\n",
        "     Regression?\n",
        "  - Scaling (standardizing or normalizing) is important in\n",
        "    Multiple Linear Regression for both practical and statistical reasons.\n",
        "\n",
        "23.  What is polynomial regression?\n",
        "\n",
        "  - Polynomial regression fits a nonlinear curve to the data by\n",
        "    adding polynomial terms (squared, cubed, etc.) of the independent variable(s) to a linear regression model.\n",
        "\n",
        "24. How does polynomial regression differ from linear\n",
        "    regression?\n",
        "  - Linear Regression | Polynomial Regression\n",
        "    Models a straight-line relationship between the independent and dependent variable. | Models a curved (nonlinear) relationship by including powers of the predictor variable(s).\n",
        "    Example: y=β0+β1xy = \\beta_0 + \\beta_1 xy=β0​+β1​x | Example: y=β0+β1x+β2x2+β3x3y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + \\beta_3 x^3y=β0​+β1​x+β2​x2+β3​x3\n",
        "\n",
        "25. When is polynomial regression used?\n",
        "\n",
        "  - Polynomial regression is used when you want to model   non-linear relationships between the independent and  dependent variables, but still want to keep the simplicity of a regression model.\n",
        "\n",
        "26. What is the general equation for polynomial regression?\n",
        "\n",
        " - The general equation for polynomial regression is an\n",
        "   extension of linear regression, where you add polynomial terms of the predictor variable(s).\n",
        "\n",
        "   For single-variable polynomial regression, the equation is:\n",
        "\n",
        "𝑦\n",
        "=\n",
        "𝛽\n",
        "0\n",
        "+\n",
        "𝛽\n",
        "1\n",
        "𝑥\n",
        "+\n",
        "𝛽\n",
        "2\n",
        "𝑥\n",
        "2\n",
        "+\n",
        "𝛽\n",
        "3\n",
        "𝑥\n",
        "3\n",
        "+\n",
        "⋯\n",
        "+\n",
        "𝛽\n",
        "𝑛\n",
        "𝑥\n",
        "𝑛\n",
        "+\n",
        "𝜀\n",
        "y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " x+β\n",
        "2\n",
        "​\n",
        " x\n",
        "2\n",
        " +β\n",
        "3\n",
        "​\n",
        " x\n",
        "3\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " x\n",
        "n\n",
        " +ε\n",
        "\n",
        "27. Can polynomial regression be applied to multiple variables?\n",
        "\n",
        " - Yes, polynomial regression can definitely be applied to\n",
        "   multiple variables (also known as multiple polynomial regression). When dealing with multiple predictors, you can extend the idea of polynomial regression by adding polynomial terms for each independent variable.\n",
        "\n",
        "28. What are the limitations of polynomial regression?\n",
        "\n",
        "  - While polynomial regression can be very useful for modeling\n",
        "    complex, nonlinear relationships, it comes with certain limitations.\n",
        "    i. Overfitting\n",
        "    ii. Extrapolation Issues\n",
        "    iii. Interpretability Challenges\n",
        "    iv. Multicollinearity\n",
        "    v.  Increased Complexity and Computation\n",
        "    vi. Limited to Polynomial Forms\n",
        "    vii. Sensitive to Outliers\n",
        "\n",
        "29.  What methods can be used to evaluate model fit when\n",
        "     selecting the degree of a polynomial?\n",
        "\n",
        "  - When selecting the degree of a polynomial for polynomial\n",
        "    regression, it’s crucial to evaluate the model fit effectively to avoid both underfitting (too simple a model) and overfitting (too complex a model). Here are some methods commonly used to assess the model fit:\n",
        "    i. Cross-Validation\n",
        "    ii. Train/Test Split\n",
        "    iii. Adjusted R²\n",
        "    iv. AIC (Akaike Information Criterion) and\n",
        "       BIC (Bayesian Information Criterion)\n",
        "    v. Residual Analysis\n",
        "    vi. Validation Curve (Learning Curve)\n",
        "\n",
        "30.  Why is visualization important in polynomial regression?\n",
        "\n",
        "   - visualization is important in polynomial regression:\n",
        "    i. Visualizing the Fit of the Polynomial Model\n",
        "    ii. Evaluating the Degree of the Polynomial\n",
        "    iii. Checking for Overfitting or Underfitting\n",
        "    iv. Visualizing Residuals\n",
        "    v.  Identifying Outliers or Leverage Points\n",
        "    vi. Model Comparison\n",
        "\n",
        "31. How is polynomial regression implemented in Python?\n",
        "\n",
        "  - Polynomial regression can be implemented in Python using\n",
        "    libraries such as NumPy, scikit-learn, and matplotlib.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mY7w9ifH4xtz"
      }
    }
  ]
}